{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# People Dimension Model Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import starting libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk \n",
    "\n",
    "# Set directory\n",
    "os.chdir(\"/Users/Sam Edds/datascience-challenge/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpickle file\n",
    "labeled = pd.read_pickle(\"labelled_dataset.pickle\")\n",
    "\n",
    "# Check out data\n",
    "labeled.info()\n",
    "labeled[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for and remove duplicates\n",
    "labeled = labeled.drop_duplicates(subset = ['text','labelmax'])\n",
    "len(labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "### Clean text ###\n",
    "\n",
    "# Removing stop words\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Get rid of extras, split, and join for text\n",
    "def preprocess(x):\n",
    "    x = re.sub('[^a-z\\s]', '',x.lower())                   \n",
    "    x = [w for w in x.split() if w not in stopwords]       \n",
    "    return ' '.join(x)  \n",
    "\n",
    "# Call\n",
    "labeled['text_clean'] = labeled['text'].apply(preprocess)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only obs with text\n",
    "labeled = labeled[labeled['text_clean'] != '']\n",
    "len(labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the labels\n",
    "labeled.groupby(['labelmax'])['text'].agg(['count'])\n",
    "\n",
    "# Remove null\n",
    "labeled = labeled[labeled['labelmax'] != 'null']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split pros and cons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into pros and cons\n",
    "pros_cons = [re.split(r\"con{1}s\\s\", w) for w in  labeled['text_clean']]\n",
    "\n",
    "# Check if there are just pros or just cons\n",
    "for review in pros_cons:\n",
    "    if len(review) == 1:\n",
    "        print(review)\n",
    "        # Since just pros add '' for cons\n",
    "        review.append('')\n",
    "\n",
    "# Create new variables\n",
    "labeled['pros'] = [w[0] for w in pros_cons]\n",
    "labeled['cons'] = [w[1] for w in pros_cons]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make labels numeric \n",
    "labeled.labelmax = pd.Categorical(labeled.labelmax)\n",
    "labeled['label'] = labeled.labelmax.cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N Grams with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Compute n grams from a dataframe for a given variable\n",
    "class Ngrams(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, df):\n",
    "        pass\n",
    "\n",
    "    def transform(self, df):\n",
    "        # Save name of variable to analyze\n",
    "        name = df.columns\n",
    "        # Initiate TfidfVectorizer\n",
    "        vectorizer = TfidfVectorizer(strip_accents = 'unicode', use_idf = True, \\\n",
    "                                     stop_words = 'english', analyzer = 'word', \\\n",
    "                                     ngram_range = (1, 2), max_features = 50)\n",
    "        # Fit to data\n",
    "        x_train = vectorizer.fit_transform(df[name[0]].values)\n",
    "        # Return sparse matrix\n",
    "        return x_train\n",
    "    \n",
    "    def fit(self, df, y=None):\n",
    "        # Unless error returns self\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for punctuation that may be an indicator of strong feelings about time at company\n",
    "class Punctuation(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, df):\n",
    "        pass\n",
    "    \n",
    "    def transform(self, df):\n",
    "        # Lots of exclamations\n",
    "        df_new = df[['text']].copy()\n",
    "        df_new['exclaim_many'] = df_new['text'].str.contains('[(!)]+', regex=True)*1\n",
    "\n",
    "        # Lots of punctuation\n",
    "        df_new['exclaim_q'] = df_new['text'].str.contains('[(!?)]', regex=True)*1\n",
    "\n",
    "        # Caps (need more than 10 letters) extract, expand if want to checks\n",
    "        df_new['caps'] = df_new['text'].str.contains('([A-Z]{10})', regex=True)*1\n",
    "        \n",
    "        # Drop text\n",
    "        df_new = df_new.drop(columns = ['text'], axis = 1)\n",
    "\n",
    "        return df_new\n",
    "    \n",
    "    def fit(self, df, y=None):\n",
    "        # Unless error returns self\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "\n",
    "### Positive, negative, neutral sentiment analysis for a given variable ###\n",
    "\n",
    "class SentimentAnalysis(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, df):\n",
    "        pass\n",
    "\n",
    "    def transform(self, df):\n",
    "        # Initialize\n",
    "        sent = SIA()\n",
    "        results = []\n",
    "        # Variable name to compute scores on\n",
    "        name = df.columns\n",
    "        # Make into list\n",
    "        sent_list = df[name[0]].tolist()\n",
    "        # Compute polarity score for each review and add onto df\n",
    "        for review in sent_list:\n",
    "            pol_score = sent.polarity_scores(review)\n",
    "            results.append(pol_score)\n",
    "        # Make into a pandas df\n",
    "        df_new = pd.DataFrame.from_records(results)\n",
    "        # Add suffix\n",
    "        df_new = df_new.add_suffix(name)\n",
    "\n",
    "        return df_new\n",
    "    \n",
    "    def fit(self, df, y=None):\n",
    "        # Unless error returns self\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine and run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "### Pipe different features in with a name so the step can be later called for details ###\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        # Ngrams\n",
    "        ('ngram_all', Ngrams(labeled[['text_clean']])),\n",
    "        # Sentiment\n",
    "       ('sent_pros', SentimentAnalysis(labeled[['pros']])),\n",
    "        ('sent_cons', SentimentAnalysis(labeled[['cons']])),\n",
    "        # Punctuation\n",
    "       ('punc', Punctuation(labeled)),\n",
    "\n",
    "    ])),\n",
    "     # Classifier\n",
    "     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                           alpha=1e-3, random_state=52,\n",
    "                           max_iter=10, tol=10)),])\n",
    "\n",
    "\n",
    "# Cross validation and tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'clf__alpha': (1e1, 1e3, 1e-5),\n",
    "            'clf__max_iter': (20, 30),\n",
    "}\n",
    "\n",
    "\n",
    "# Find best model\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, iid=False, n_jobs=-1, refit = True)\n",
    "grid_search.fit(labeled, labeled['label'])\n",
    "\n",
    "# Print it\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pipeline on best model\n",
    "pipeline = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        # Ngrams\n",
    "        ('ngram_all', Ngrams(labeled[['text_clean']])),\n",
    "        # Sentiment\n",
    "       ('sent_pros', SentimentAnalysis(labeled[['pros']])),\n",
    "        ('sent_cons', SentimentAnalysis(labeled[['cons']])),\n",
    "        # Punctuation\n",
    "       ('punc', Punctuation(labeled))\n",
    "    ])),\n",
    "    # Classifier\n",
    "     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                          alpha=grid_search.best_params_['clf__alpha'], random_state=52,\n",
    "                           max_iter=grid_search.best_params_['clf__max_iter'], tol=10)),])\n",
    "\n",
    "# Run\n",
    "pipeline.fit(labeled, labeled['label'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "### Read in unlabeled data and make into a list of dictionaries ###\n",
    "\n",
    "# Initialize\n",
    "all_reviews = []\n",
    "# Set file location\n",
    "for file in os.listdir(\"/Users/Sam Edds/datascience-challenge/data/unlabelled-dataset/\"):\n",
    "    full_filename = \"%s/%s\" % (\"/Users/Sam Edds/datascience-challenge/data/unlabelled-dataset/\", file)\n",
    "    # append each set of reviews to a list\n",
    "    with open(full_filename,'r') as indv_review:\n",
    "        dict = json.load(indv_review)\n",
    "        all_reviews.append(dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make into a dataframe ###\n",
    "\n",
    "# Initialize df\n",
    "unlabeled = pd.DataFrame(all_reviews[0])\n",
    "# Count to keep track of the company...this could be used in the future\n",
    "unlabeled['n'] = 0\n",
    "\n",
    "# For each company after make a separate dataframe to append\n",
    "for i in range(1,len(all_reviews)):\n",
    "    init_df = pd.DataFrame(all_reviews[i])\n",
    "    init_df['n'] = i\n",
    "    unlabeled = unlabeled.append(init_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clean to match test file ###\n",
    "\n",
    "# Since I didn't use create advice or have title or rating remove these\n",
    "unlabeled = unlabeled.drop(columns = ['advice', 'rating', 'title'], axis = 1)\n",
    "unlabeled = unlabeled.reset_index(drop = True)\n",
    "\n",
    "# Clean text using preprocesser from beginning of code\n",
    "unlabeled['text_clean'] = unlabeled['text'].apply(preprocess)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Out-Of-Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels\n",
    "pred_unlabeled = pipeline.predict(unlabeled)   \n",
    "\n",
    "# Add to df\n",
    "unlabeled['label'] = pred_unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the labels \n",
    "print(labeled.labelmax.unique())\n",
    "print(labeled.label.unique())\n",
    "\n",
    "# Add them\n",
    "# Add corresponding category\n",
    "unlabeled['category'] = np.where(unlabeled['label'] == 0, \"adaptability\",\n",
    "                                np.where(unlabeled['label'] == 1, \"collaboration\",\n",
    "                                        np.where(unlabeled['label'] == 2, \"customer\",\n",
    "                                            np.where(unlabeled['label'] == 3, \"detail\",\n",
    "                                                    np.where(unlabeled['label'] == 4, \"integrity\",\"result\")))))\n",
    "\n",
    "# Output to csv\n",
    "unlabeled.to_csv(\"unlabeled_review_predictions_SE.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at how we overpredict likely on 2 and 5\n",
    "unlabeled.groupby(['category'])['label'].agg(['count'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
